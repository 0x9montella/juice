//! Provides the container of a Deep Learning Network
//!
//! Holds all the information about its Layers, how they are connected
//! and how the [forward][1] and [backward][2] steps should be
//! handeled and optimized (e.g. skipping layers).
//!
//! [1]: ./struct.Network.html#method.forward
//! [2]: ./struct.Network.html#method.backward
//!
//! If you are looking to train/test a network, [Solver][3] is usually a better
//! entry point.
//!
//! ## Development
//!
//! Currently only new networks can be created with [from_config][4].
//! In the future there should also be a way to load networks with saved
//! weights from a file.
//! [Issue #14][5].
//!
//! Currently the layers are stored in an array and the metadata in another
//! array with matching
//! indices.
//! In the future we would like to take the metadata into the Layer struct.
//! [Issue #16][6].
//!
//! [3]: ../solver/index.html
//! [4]: #method.from_config
//! [5]: https://github.com/autumnai/leaf/issues/14
//! [6]: https://github.com/autumnai/leaf/issues/16
//!
//! ## Glossary
//! ### Input Layers / Blobs
//!
//! A input layer is the bottom-most layer of a network.</br>
//! During a forward step the data is put into the input layer,
//! passed through all the intermediate (hidden) layers and generates a
//! result in the output layer.
//!
//! The blobs in a input layer contain externally preprocessed data that has
//! been brought
//! into a form suitable for consumption by a neural network.
//! TODO: explanation about feedforward / backpropagation
use std::collections::{HashMap, HashSet};
use std::sync::{Arc, RwLock};
use std::cmp;
use math::*;
use shared_memory::*;
use layer::{ILayer, Layer};
use layer::{LayerConfig, WeightConfig};
use phloem::Blob;

#[derive(Debug)]
/// Defines a [Network][1] that contains the [Layers][2] and [Blobs][3] that store
/// the intermediate results between the layers which are generated by [forward][4]/[backward][5].
/// [1]: https://en.wikipedia.org/wiki/Artificial_neural_network
/// [2]: ../layer/struct.Layer.html
/// [3]: ../../phloem/blob/struct.Blob.html
/// [4]: ./struct.Network.html#method.forward
/// [5]: ./struct.Network.html#method.backward
///
/// It is also responsible for setting up the connections between the layers.
/// A Network is usually used together with a [Solver][6] to optimize the networks' weights.
///
/// [6]: ../solver/struct.Solver.html
pub struct Network<'a> {
    /// Identifies the Network
    ///
    /// The name is mainly used for logging purposes.
    pub name: String,
    layers: Vec<Layer<'a>>,
    layer_names: Vec<String>,
    layer_names_index: HashMap<String, usize>,
    layer_need_backwards: Vec<bool>,

    blobs: Vec<ArcLock<HeapBlob>>, // the blobs storing intermediate results between the layer.
    blob_names: Vec<String>,
    blob_names_index: HashMap<String, usize>,
    blob_need_backwards: Vec<bool>,

    output_blobs: Vec<ArcLock<HeapBlob>>,
    output_blob_indices: Vec<usize>,

    // stores the vectors containing the tops (output in forward) for each layer
    // (only references to the blobs)
    top_vecs: Vec<Vec<ArcLock<HeapBlob>>>,
    top_id_vecs: Vec<Vec<usize>>,
    // stores the vectors containing the bottoms (input in forward) for each layer
    bottom_vecs: Vec<Vec<ArcLock<HeapBlob>>>,
    bottom_id_vecs: Vec<Vec<usize>>,
    bottom_need_backwards: Vec<Vec<bool>>,

    input_blobs: Vec<ArcLock<HeapBlob>>,
    input_blob_indices: Vec<usize>,

    // Vector of weight in the loss (or objective) function of each net blob, indexed by blob_id.
    blob_loss_weights: Vec<f32>,

    weight_id_vecs: Vec<Vec<usize>>,
    weight_owners: Vec<Option<usize>>,
    weight_display_names: Vec<String>,
    weight_layer_indices: Vec<(usize, usize)>,
    weight_names_index: HashMap<String, usize>,

    /// Defines the [parameters/weights][1] of the network.
    /// [1]: https://en.wikipedia.org/wiki/Synaptic_weight
    ///
    /// Parameters are currently in the process of being renamed to weights throughout the codebase.
    /// [Issue #17](https://github.com/autumnai/leaf/issues/17)
    weights: Vec<ArcLock<HeapBlob>>,
    learnable_weights: Vec<ArcLock<HeapBlob>>,
    learnable_weight_ids: Vec<usize>,

    weights_lr: Vec<Option<f32>>,
    weights_weight_decay: Vec<Option<f32>>,
}

impl<'a> Default for Network<'a> {
    fn default() -> Network<'a> {
        Network {
            name: "".to_owned(),
            layers: vec![],
            layer_names: vec![],
            layer_names_index: HashMap::<String, usize>::new(),
            layer_need_backwards: vec![],

            blobs: vec![],
            blob_names: vec![],
            blob_names_index: HashMap::<String, usize>::new(),
            blob_need_backwards: vec![],

            output_blobs: vec![],
            output_blob_indices: vec![],

            top_vecs: vec![],
            top_id_vecs: vec![],

            bottom_vecs: vec![],
            bottom_id_vecs: vec![],
            bottom_need_backwards: vec![],

            input_blobs: vec![],
            input_blob_indices: vec![],

            blob_loss_weights: vec![],

            weight_id_vecs: vec![],
            weight_owners: vec![],
            weight_display_names: vec![],
            weight_layer_indices: vec![],
            weight_names_index: HashMap::<String, usize>::new(),

            weights: vec![],
            learnable_weights: vec![],
            learnable_weight_ids: vec![],

            weights_lr: vec![],
            weights_weight_decay: vec![],
        }
    }
}

impl<'a> Network<'a> {
    /// Creates a Network from a [NetworkConfig][1].
    /// [1]: ./struct.NetworkConfig.html
    ///
    /// ## Examples
    ///
    /// ```
    /// # use leaf::network::*;
    /// let cfg = NetworkConfig::default();
    /// Network::from_config(&cfg);
    /// ```
    pub fn from_config(param: &NetworkConfig) -> Network {
        let mut network = Network::default();
        network.init(param);
        network
    }

    /// Initializes a network.
    ///
    /// Sets up the whole structure of the network. It reads the supplied [NetworkConfig][1],
    /// appends the top and bottom blobs to each layer and determines if the backpropagation has
    /// to be executed for each blob and layer.
    ///
    /// [1]: ./struct.NetworkConfig.html
    fn init(&mut self, in_config: &'a NetworkConfig) {
        let config = in_config.clone();
        let available_blobs = &mut HashSet::new();
        let blob_name_to_idx = &mut HashMap::<String, usize>::new();
        for (input_id, _) in config.inputs.iter().enumerate() {
            self.append_top(config,
                            None,
                            input_id,
                            Some(available_blobs),
                            Some(blob_name_to_idx));
        }

        self.resize_vecs(config.layers.len());

        for (layer_id, _) in config.inputs.iter().enumerate() {
            self.init_layer(layer_id, config, available_blobs, blob_name_to_idx);
        }

        // Go through the net backwards to determine which blobs contribute to the
        // loss.  We can skip backward computation for blobs that don't contribute
        // to the loss.
        // Also checks if all bottom blobs don't need backward computation (possible
        // because the skip_propagate_down config) and so we can skip backward
        // computation for the entire layer
        let blobs_under_loss = &mut HashSet::<String>::new();
        let blobs_skip_backp = &mut HashSet::<String>::new();
        // get mutable references to struct fields because Rust doesn't support
        // partially borrowed structs
        let layer_need_backwards = &mut self.layer_need_backwards.clone();
        let bottom_need_backwards = &mut self.bottom_need_backwards.clone();
        for (layer_id, _) in self.layers.iter().rev().enumerate() {
            self.init_backprop(layer_id,
                               layer_need_backwards,
                               bottom_need_backwards,
                               blobs_under_loss,
                               blobs_skip_backp);
        }

        if config.force_backward {
            self.init_force_backward();
        }

        // In the end, all remaining blobs are considered output blobs.
        for available_blob in available_blobs.iter() {
            info!("This network produces output {}", available_blob);
            let id = blob_name_to_idx[available_blob];
            self.output_blobs.push(self.blobs[id].clone());
            self.output_blob_indices.push(id);
        }

        // setup names->idx
        for (blob_id, blob_name) in self.blob_names.iter().enumerate() {
            self.blob_names_index.insert(blob_name.clone(), blob_id);
        }
        for (layer_id, layer_name) in self.layer_names.iter().enumerate() {
            self.layer_names_index.insert(layer_name.clone(), layer_id);
        }

        self.share_weights();

        info!("Network initialization done.");
    }

    /// Initializes a single layer of the network.
    ///
    /// Appends [top][1] and [bottom blobs][2] to the [Layer][3]. Apart from explicitly named
    /// top blobs it will also append anonymous top blobs that are required by the specific
    /// [Layer implemenations][4]. It also sets up the [loss weights],
    /// and backpropagation flags.
    ///
    /// [1]: ../layer/index.html
    /// [2]: ../layer/index.html
    /// [3]: ../layer/struct.Layer.html
    /// [4]: ../layers/index.html
    fn init_layer(&mut self,
                  layer_id: usize,
                  config: &'a NetworkConfig,
                  available_blobs: &mut HashSet<String>,
                  blob_name_to_idx: &mut HashMap<String, usize>) {
        // Caffe
        // bool share_from_root = !Caffe::root_solver()
        //     && root_net_->layers_[layer_id]->ShareInParallel();
        // // Inherit mode from net if unset.
        // if (!param.layer(layer_id).has_mode()) {
        //   param.mutable_layer(layer_id)->set_mode(mode_);
        // }

        // Setup layer.
        let layer_config = (&config.layers[layer_id]).clone(); // TODO: should be safer
        if !layer_config.check_propagate_down_len() {
            // TODO: move layer validation to layer
            error!("propagate_down config must be specified either 0 or bottom_size times")
        }

        // Caffe
        // if (share_from_root) {
        //   LOG(INFO) << "Sharing layer " << layer_param.name() << " from root net";
        //   layers_.push_back(root_net_->layers_[layer_id]);
        //   layers_[layer_id]->SetShared(true);
        // else {
        {
            self.layers.push(Layer::from_config(&layer_config));
        }
        self.layer_names.push(layer_config.name.clone());
        info!("Creating Layer {}", layer_config.name.clone());
        let mut need_backward = false;

        // Figure out this layer's input and output

        for bottom_id in 0..(layer_config.bottoms_len() - 1) {
            let blob_id = self.append_bottom(config,
                                             layer_id,
                                             bottom_id,
                                             available_blobs,
                                             blob_name_to_idx);

            // If a blob needs backward, this layer should provide it.
            need_backward |= self.blob_need_backwards[blob_id];
        }
        let num_top = layer_config.tops_len();
        for top_id in 0..(num_top - 1) {
            self.append_top(config,
                            Some(layer_id),
                            top_id,
                            Some(available_blobs),
                            Some(blob_name_to_idx))
        }

        // If the layer specifies that AutoTopBlobs() -> true and the LayerParameter
        // specified fewer than the required number (as specified by
        // ExactNumTopBlobs() or MinTopBlobs()), allocate them here.
        let auto_top_blobs = self.layers.get(layer_id).unwrap().worker.auto_top_blobs();
        let min_top_blobs = self.layers.get(layer_id).unwrap().worker.min_top_blobs();
        let exact_num_top_blobs = self.layers.get(layer_id).unwrap().worker.exact_num_top_blobs();
        if auto_top_blobs {
            let needed_num_top = cmp::max(min_top_blobs, exact_num_top_blobs);
            for _ in 0..(needed_num_top - num_top) {
                // Add "anonymous" top blobs -- do not modify available_blobs or
                // blob_name_to_idx as we don't want these blobs to be usable as input
                // to other layers.
                info!("Adding anonymous top blob");
                self.append_top(config, Some(layer_id), num_top, None, None);
            }
        }

        // After this layer is connected, set it up.
        // Caffe
        // if (share_from_root) {
        //   // Set up size of top blobs using root_net_
        //   const vector<Blob<Dtype>*>& base_top = root_net_->top_vecs_[layer_id];
        //   const vector<Blob<Dtype>*>& this_top = this->top_vecs_[layer_id];
        //   for (int top_id = 0; top_id < base_top.size(); ++top_id) {
        //     this_top[top_id]->ReshapeLike(*base_top[top_id]);
        //     LOG(INFO) << "Created top blob " << top_id << " (shape: "
        //         << this_top[top_id]->shape_string() <<  ") for shared layer "
        //         << layer_param.name();
        //   }
        // } else {
        {
            // layers_[layer_id]->SetUp(bottom_vecs_[layer_id], top_vecs_[layer_id]);
            // TODO
            // self.layers[layer_id].set_up(self.bottom_vecs[layer_id],
            // self.top_vecs[layer_id]);
        }

        info!("Setting up {}", self.layer_names[layer_id]);
        let layer = self.layers.get(layer_id).unwrap(); // TODO: should be safer?
        for top_id in 0..(self.top_vecs[layer_id].len() - 1) {
            if self.blob_loss_weights.len() <= self.top_id_vecs[layer_id][top_id] {
                self.blob_loss_weights.resize(self.top_id_vecs[layer_id][top_id] + 1, 0f32);
            }
            self.blob_loss_weights[self.top_id_vecs[layer_id][top_id]] = *layer.loss(top_id).unwrap();
            info!("Top shape: {}",
                  self.top_vecs[layer_id][top_id].read().unwrap().shape_string());
            info!("   with loss weight {}", *layer.loss(top_id).unwrap());
        }

        // TODO: only needed if we allow blobs to be passed along in the layer_config
        // const int param_size = layer_param.param_size();
        // const int num_param_blobs = layers_[layer_id]->blobs().size();
        // CHECK_LE(param_size, num_param_blobs)
        //     << "Too many params specified for layer " << layer_param.name();
        // ParamSpec default_param_spec;
        // for (int param_id = 0; param_id < num_param_blobs; ++param_id) {
        //   const ParamSpec* param_spec = (param_id < param_size) ?
        //       &layer_param.param(param_id) : &default_param_spec;
        //   const bool param_need_backward = param_spec->lr_mult() != 0;
        //   need_backward |= param_need_backward;
        //   layers_[layer_id]->set_param_propagate_down(param_id,
        //                                               param_need_backward);
        // }
        // for (int param_id = 0; param_id < num_param_blobs; ++param_id) {
        //   AppendParam(param, layer_id, param_id);
        // }

        // Finally, set the backward flag
        self.layer_need_backwards.push(need_backward);
        if need_backward {
            for top_id in 0..(self.top_id_vecs[layer_id].len() - 1) {
                self.blob_need_backwards[self.top_id_vecs[layer_id][top_id]] = true;
            }
        }
    }

    /// Initializes network for [backpropagation][1]
    /// [1]: https://en.wikipedia.org/wiki/Backpropagation
    ///
    /// Go through all the blobs of a layer to determine which blobs contribute to the
    /// loss of the next layer. We can skip backward computation for blobs that don't contribute
    /// to the loss.
    /// If all of the blobs skip backpropagation we set a flag to skip backpropagation
    /// of the whole layer.
    fn init_backprop(&self,
                     layer_id: usize,
                     layer_need_backwards: &mut Vec<bool>,
                     bottom_need_backwards: &mut Vec<Vec<bool>>,
                     blobs_under_loss: &mut HashSet<String>,
                     blobs_skip_backp: &mut HashSet<String>) {
        let mut layer_contributes_loss = false;
        let mut layer_skip_propagate_down = true;
        for (top_id, _) in self.top_vecs[layer_id].iter().enumerate() {
            let blob_name = self.blob_names[self.top_id_vecs[layer_id][top_id]].clone();

            // layer is a loss layer or under a loss layer
            if self.layers[layer_id].loss(top_id).is_some() || blobs_under_loss.contains(&blob_name) {
                layer_contributes_loss = true;
            }
            // layer is not marked to skip backprop TODO: confirm doc
            if !blobs_skip_backp.contains(&blob_name) {
                layer_skip_propagate_down = false;
            }
            // layer contributes loss to some
            if layer_contributes_loss && !layer_skip_propagate_down {
                break;
            }
        }

        // If this layer can skip backward computation, also all his bottom blobs
        // don't need backpropagation
        if layer_need_backwards[layer_id] && layer_skip_propagate_down {
            layer_need_backwards[layer_id] = false;
            for (bottom_id, _) in self.bottom_vecs[layer_id].iter().enumerate() {
                bottom_need_backwards[layer_id][bottom_id] = false;
            }
        }
        // layer doesn't contribute loss so it does not need to be backpropagated
        if !layer_contributes_loss {
            layer_need_backwards[layer_id] = false;
        }
        // if (Caffe::root_solver()) { // Caffe
        {
            info!("{} needs backward computation: {}",
                  self.layer_names[layer_id],
                  self.layer_need_backwards[layer_id]);
        }

        for (bottom_id, _) in self.bottom_vecs[layer_id].iter().enumerate() {
            let blob_name = &self.blob_names[self.bottom_id_vecs[layer_id][bottom_id]];
            if layer_contributes_loss {
                blobs_under_loss.insert(blob_name.clone());
            } else {
                bottom_need_backwards[layer_id][bottom_id] = false;
            }
            if !self.bottom_need_backwards[layer_id][bottom_id] {
                blobs_skip_backp.insert(blob_name.clone());
            }
        }
    }

    /// Set [backpropagation][1] flags to force all layers to backpropagate.
    /// [1]: https://en.wikipedia.org/wiki/Backpropagation
    ///
    /// Is executed during Network initalization if [NetworkConfig][2].force_backward is true.
    /// Forcing backpropagation is useful for debugging.
    fn init_force_backward(&mut self) {
        for (layer_id, layer) in self.layers.iter_mut().enumerate() {
            self.layer_need_backwards[layer_id] = true;
            for (bottom_id, _) in self.bottom_need_backwards[layer_id].clone().iter().enumerate() {
                self.bottom_need_backwards[layer_id][bottom_id] =
                    *self.bottom_need_backwards[layer_id]
                         .get(bottom_id)
                         .unwrap_or(&layer.worker.allow_force_backward(bottom_id));
                self.blob_need_backwards[self.bottom_id_vecs[layer_id][bottom_id]] =
                    *self.blob_need_backwards
                         .get(self.bottom_id_vecs[layer_id][bottom_id])
                         .unwrap_or(&self.bottom_need_backwards[layer_id][bottom_id])
            }
            for (weight_id, _) in layer.blobs.clone().iter().enumerate() {
                layer.set_weight_propagate_down(weight_id, true);
            }
        }
    }

    /// Resize Vectors that hold the [HeapBlob][1] references and the layer metadata.
    /// [1]: ../shared_memory/type.HeapBlob.html
    ///
    /// Used during Network initalization.
    /// It is unclear if this provides any (speed) benefit since the Vecs only hold
    /// references, so reallocation should be cheap.
    fn resize_vecs(&mut self, new_len: usize) {
        self.bottom_vecs.resize(new_len, vec![Arc::new(RwLock::new(Box::new(Blob::new())))]);
        self.top_vecs.resize(new_len, vec![Arc::new(RwLock::new(Box::new(Blob::new())))]);
        self.bottom_id_vecs.resize(new_len, vec![0]);
        self.top_id_vecs.resize(new_len, vec![0]);
        self.weight_id_vecs.resize(new_len, vec![0]);
        self.bottom_need_backwards.resize(new_len, vec![false]);
    }

    /// Share weights among multiple layers.
    ///
    /// Shared weights are usually used for [Siamese networks][1]
    ///
    /// [1]: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.28.4792
    fn share_weights(&mut self) {
        // Caffe / not sure if ported correctly
        // for (int i = 0; i < params_.size(); ++i) {
        //     if (param_owners_[i] < 0) { continue; }
        //     params_[i]->ShareData(*params_[param_owners_[i]]);
        //     params_[i]->ShareDiff(*params_[param_owners_[i]]);
        // }
        for (i, _) in self.weights.clone().iter().enumerate() {
            if let Some(j) = self.weight_owners[i] {
                assert!(self.weights[i].read().unwrap().cpu_data().capacity() ==
                        self.weights[j].read().unwrap().cpu_data().capacity());
                self.weights[i] = self.weights[j].clone(); // sharing whole blob?
            }
        }
    }

    /// ???
    ///
    /// TODO: [DOC] Why? What is the purpose of this?
    fn append_top(&mut self,
                  config: &NetworkConfig,
                  layer_id: Option<usize>,
                  top_id: usize,
                  available_blobs: Option<&mut HashSet<String>>,
                  blob_name_to_idx: Option<&mut HashMap<String, usize>>) {
        let mut layer_config: Option<&LayerConfig> = None;
        if layer_id.is_some() {
            layer_config = config.layer(layer_id.unwrap());
        }

        let blob_name: String;
        match layer_config {
            Some(layer_config) => {
                if layer_config.top(top_id).is_some() {
                    blob_name = String::from(layer_config.top(top_id).unwrap().clone());
                } else {
                    blob_name = "(automatic)".to_owned();
                }
            }
            None => {
                blob_name = String::from(config.input(top_id).unwrap().clone());
            }
        }

        if blob_name_to_idx.is_some() && layer_config.is_some() && layer_config.unwrap().bottom(top_id).is_some() &&
           *layer_config.unwrap().bottom(top_id).unwrap() == blob_name {
            info!("{} -> {} (in-place)", layer_config.unwrap().name, blob_name);
            let idx = blob_name_to_idx.unwrap()[&blob_name];
            let blob = self.blobs[idx].clone();
            self.top_vecs[layer_id.unwrap()].push(blob);
            self.top_id_vecs[layer_id.unwrap()].push(idx);
        } else if blob_name_to_idx.is_some() && blob_name_to_idx.as_ref().unwrap().get(&blob_name).is_some() {
            // If we are not doing in-place computation but have duplicated blobs, raise an
            // error.
            error!("Top blob {} produced by multiple sources.", blob_name);
        } else {
            // if (Caffe::root_solver()) {
            if true {
                if layer_config.is_some() {
                    info!("{} -> {}", layer_config.unwrap().name, blob_name);
                }
                info!("Input {} -> {}", top_id, blob_name);
            }

            let blob_pointer: ArcLock<HeapBlob> = Arc::new(RwLock::new(Box::new(Blob::new())));
            let blob_id = self.blobs.len();
            self.blobs.push(blob_pointer.clone());
            self.blob_names.push(blob_name.to_owned());
            self.blob_need_backwards.push(false);
            if blob_name_to_idx.is_some() {
                blob_name_to_idx.unwrap().insert(blob_name.to_owned(), blob_id);
            }

            match layer_id {
                None => {
                    // Set the (explicitly specified) dimensions of the input blob.
                    blob_pointer.write().unwrap().reshape(config.input_shape(top_id).unwrap().clone());

                    self.input_blob_indices.push(blob_id);
                    self.input_blobs.push(blob_pointer);
                }
                Some(layer_id) => {
                    self.top_id_vecs[layer_id].push(blob_id);
                    self.top_vecs[layer_id].push(blob_pointer);
                }
            }
        }
        if available_blobs.is_some() {
            available_blobs.unwrap().insert(blob_name.to_owned());
        }
    }

    /// Append blob as [bottom blob][1] to a [Layer][2].
    /// [1]: ../layer/index.html
    /// [2]: ../layer/struct.Layer.html
    ///
    /// During network initalization the blobs will be appended to the [Layer][2]s as per their
    /// [LayerConfig][3]. It is also determined if a bottom blob skips backpropagation
    /// from [LayerConfig.propagate_down][3] (see also [init_backprop][5]).
    ///
    /// Currently these things are tracked in metadata arrays: [Issue #16]][4].
    ///
    /// [3]: ../layer/struct.LayerConfig.html
    /// [4]: https://github.com/autumnai/leaf/issues/16
    /// [5]: #method.init_backprop
    fn append_bottom(&mut self,
                     config: &NetworkConfig,
                     layer_id: usize,
                     bottom_id: usize,
                     available_blobs: &mut HashSet<String>,
                     blob_name_to_idx: &mut HashMap<String, usize>)
                     -> usize {
        let layer_config = config.layer(layer_id).unwrap();
        let blob_name = layer_config.bottom(bottom_id).unwrap();

        if !available_blobs.contains(blob_name) {
            error!("Unknown bottom blob {} (layer '{}', bottom index {})",
                   blob_name,
                   layer_config.name,
                   bottom_id);
        }

        let blob_id = blob_name_to_idx[blob_name];
        info!("{} <- {}", self.layer_names[layer_id], blob_name);

        self.bottom_vecs[layer_id].push(self.blobs[blob_id].clone());
        self.bottom_id_vecs[layer_id].push(blob_id);
        available_blobs.remove(blob_name);

        let mut propagate_down = true;
        // Check if the backpropagation on bottom_id should be skipped
        if !layer_config.propagate_down.is_empty() {
            propagate_down = layer_config.propagate_down[bottom_id];
        }
        let need_backward = self.blob_need_backwards[blob_id] && propagate_down;
        self.bottom_need_backwards[layer_id].push(need_backward);

        blob_id
    }

    /// Append a weight blob to the network.
    ///
    /// During network initalization weight blobs are appended to the correct layers.
    /// If a layer's [LayerConfig][1] states that the weights are shared,
    /// this function also makes sure to set a reference to the other weight blob instead of
    /// allocating a new one.
    ///
    /// [1]: ../layer/struct.LayerConfig.html
    fn append_weight(&mut self, config: &NetworkConfig, layer_id: usize, weight_id: usize) {
        let layer_config = self.layers[layer_id].config.clone();
        let weights_len = self.weights.len();
        let weight_name = if weights_len > weight_id {
            layer_config.param(weight_id).unwrap().name.clone()
        } else {
            "".to_owned()
        };

        // use weight_name (or weight_id as a fallback) as display_name
        if !weight_name.is_empty() {
            self.weight_display_names.push(weight_name.clone());
        } else {
            self.weight_display_names.push(format!("{}", weight_id));
        }

        // add to tracking vectors
        let net_weight_id = weights_len;
        self.weights.push(self.layers[layer_id].blobs[weight_id].clone());
        self.weight_id_vecs[layer_id].push(net_weight_id);
        self.weight_layer_indices.push((layer_id, weight_id));

        let mut weight_config = &WeightConfig::default();
        if layer_config.params_len() > weight_id {
            weight_config = layer_config.param(weight_id).unwrap();
        }
        // This layer "owns" this weight blob -- it is either anonymous
        // (i.e., not given a weight_name) or explicitly given a name that we
        // haven't already seen.
        if weight_name.is_empty() || !self.weight_names_index.contains_key(&weight_name) {
            self.weight_owners.push(None);
            if !weight_name.is_empty() {
                self.weight_names_index.insert(weight_name.clone(), net_weight_id);
            }
            let learnable_weight_id = self.learnable_weights.len();
            self.learnable_weights.push(self.weights[net_weight_id].clone());
            self.learnable_weight_ids.push(learnable_weight_id);
            //     has_weights_lr_.push_back(weight_config->has_lr_mult());
            //     has_params_decay_.push_back(param_spec->has_decay_mult());
            self.weights_lr.push(weight_config.lr_mult.clone());
            self.weights_weight_decay.push(weight_config.decay_mult.clone());
        } else {
            // Named weight blob with name we've seen before: share weights

            let owner_net_weight_id = *self.weight_names_index.get(&weight_name).unwrap();
            self.weight_owners.push(Some(owner_net_weight_id));
            let (owner_layer_id, owner_weight_id) = self.weight_layer_indices[owner_net_weight_id];
            info!("Sharing weights '{}' owned by layer '{}', weight index {}",
                  weight_name.clone(),
                  self.layer_names[owner_layer_id],
                  owner_weight_id);
            let this_blob = self.layers[layer_id].blobs[weight_id].clone();
            let owner_blob = self.layers[owner_layer_id].blobs[owner_weight_id].clone();
            // can only share weights if blobs match by shape or capacity
            if weights_len > weight_id {
                if let Err(e) = layer_config.param(weight_id)
                                            .unwrap()
                                            .check_dimensions(&this_blob.read().unwrap(),
                                                              &owner_blob.read().unwrap(),
                                                              weight_name.clone(),
                                                              self.layer_names[owner_layer_id].clone(),
                                                              self.layer_names[layer_id].clone()) {
                    error!("{}", e)
                }
            }

            let learnable_weight_id = self.learnable_weight_ids[owner_net_weight_id];
            self.learnable_weight_ids.push(learnable_weight_id);
            // can only share parameters if both have same lr_mult
            if let Some(lr_mult) = weight_config.lr_mult {
                if let Some(owner_lr_mult) = self.weights_lr[learnable_weight_id] {
                    if !lr_mult.eq(&owner_lr_mult) {
                        error!("Shared param '{}' has mismatched lr_mult.",
                               weight_name.clone());
                    }
                } else {
                    self.weights_lr[learnable_weight_id] = weight_config.lr_mult;
                }
            }
            // can only share weights if both have same decay_mult
            if let Some(decay_mult) = weight_config.decay_mult {
                if let Some(owner_decay_mult) = self.weights_weight_decay[learnable_weight_id] {
                    if !decay_mult.eq(&owner_decay_mult) {
                        error!("Shared param '{}' has mismatched decay_mult.",
                               weight_name.clone());
                    }
                } else {
                    self.weights_weight_decay[learnable_weight_id] = weight_config.decay_mult;
                }
            }
        }
    }


    /// Computes [forward][1] and [backward][2] step for the network and returns [the total loss.][3]
    /// [1]: #method.forward
    /// [2]: #method.backward
    /// [3]: http://caffe.berkeleyvision.org/tutorial/loss.html
    ///
    /// Used by the [Solver][4] to conveniently compute one [forward- and one backward-propagation
    /// step][5] together, which is all the network has to do while training it.
    ///
    /// [4]: ../solver/struct.Solver.html
    /// [5]: https://en.wikipedia.org/wiki/Backpropagation#Phase_1:_Propagation
    pub fn forward_backward(&mut self, bottom: &[ArcLock<HeapBlob>]) -> f32 {
        let loss = &mut 0f32;

        self.forward(bottom, loss);
        // self.backward();

        *loss
    }

    /// Copies supplied [input Blobs][1] into the network, computes [forward step][2] for the
    /// network and returns [the output blobs.][3].
    /// [1]: ./index.html#input-layers--blobs
    /// [2]: https://en.wikipedia.org/wiki/Feedforward_neural_network
    /// [3]: http://caffe.berkeleyvision.org/tutorial/loss.html
    ///
    /// Does not actually copy data, only references to the input blobs.
    ///
    /// This is the go-to if you just want to feed data to your network and get the corresponding
    /// output.
    pub fn forward(&mut self, input: &[ArcLock<HeapBlob>], loss: &mut f32) -> &Vec<ArcLock<HeapBlob>> {
        for (i, inp) in input.iter().enumerate() {
            self.input_blobs[i] = inp.clone();
        }

        self.forward_prefilled(Some(loss))
    }

    /// Computes [forward step][1] for a network whose [input blob][2] references have been set
    /// and returns [the output blobs.][3]
    /// [1]: https://en.wikipedia.org/wiki/Feedforward_neural_network
    /// [2]: ./index.html#input-layers--blobs
    /// [3]: http://caffe.berkeleyvision.org/tutorial/loss.html
    ///
    /// Can be used if you need more control over how to put data into the network (debugging),
    /// otherwise [forward][4] is the prefered method to forward through the whole network.
    ///
    /// [4]: #method.forward
    pub fn forward_prefilled(&mut self, loss: Option<&mut f32>) -> &Vec<ArcLock<HeapBlob>> {
        let end = self.layers.len() - 1;
        match loss {
            Some(loss_result) => {
                // not sure if loss_result will really be changed
                *loss_result = self.forward_from_to(0, end);
            }
            None => {
                self.forward_from_to(0, end);
            }
        }

        &self.output_blobs
    }

    /// Compute [forward step][1] for a part of (or the whole) network and returns the [total loss][2].
    /// [1]: https://en.wikipedia.org/wiki/Feedforward_neural_network
    /// [2]: http://caffe.berkeleyvision.org/tutorial/loss.html
    ///
    /// Computes the forward step from the layer with index `start` to the layer with index `end`
    /// and return the total [scalar loss][2] over all loss layers.
    ///
    /// If you want to compute a foward step for the whole layer
    /// you should use [forward_prefilled][3].
    /// Computing a forward on a part of the network is usually only done for debugging purposes.
    ///
    /// [3]: #method.forward_prefilled
    pub fn forward_from_to(&mut self, start: usize, end: usize) -> f32 {
        assert!(end < self.layers.len());

        let mut loss = 0f32;

        //  Caffe
        //   if (debug_info_) {
        //     for (int i = 0; i < net_input_blobs_.size(); ++i) {
        //       InputDebugInfo(i);
        //     }
        //   }

        for i in start..end {
            loss += self.layers[i].worker.forward(&self.bottom_vecs[i], &mut self.top_vecs[i]);
            // if (debug_info_) { ForwardDebugInfo(i); }  // Caffe
        }

        loss
    }

    /// Clears the [weights][1] diffs and zero-inits them.
    /// [1]: https://en.wikipedia.org/wiki/Synaptic_weight
    ///
    /// The diffs for the weights accumulate over the backpropagation steps of
    /// a [Solver][2] minibatch and are cleared between each minibatch
    /// to start over with a clean slate.
    ///
    /// [2]: ../solver/struct.Solver.html
    pub fn clear_weight_diffs(&mut self) {
        for weight_blob in &mut self.learnable_weights.iter() {
            for p in weight_blob.write().unwrap().mutable_cpu_diff().iter_mut() {
                *p = 0f32;
            }
        }
    }

    /// Updates the [weights][1] with the weight update computed by the [Solver][2].
    /// [1]: https://en.wikipedia.org/wiki/Synaptic_weight
    /// [2]: ../solver/struct.Solver.html
    ///
    /// Updating the weights is the last step of computing a [Solver][2] minibatch.
    /// The update value is computed in previous steps according to the [learning rate policy][3]
    ///
    /// [3]: ../solver/enum.LRPolicy.html
    pub fn update_weights(&mut self) {
        for weight_blob in &self.learnable_weights {
            leaf_cpu_axpy(&-1f32,
                          weight_blob.read().unwrap().cpu_diff(),
                          weight_blob.write().unwrap().mutable_cpu_data());
        }
    }

    #[allow(missing_docs)]
    pub fn learnable_weights(&self) -> &Vec<ArcLock<HeapBlob>> {
        &self.learnable_weights
    }
}

#[derive(Debug)]
/// Defines the configuration of a network.
///
/// TODO: [DOC] When and why would you use this?
/// TODO: [DOC] What is the purpose of this configuration type?
///
/// TODO: [DOC] <Now-What> Examples
pub struct NetworkConfig {
    /// Defines the name the network.
    pub name: String,

    /// Defines the names of the [input blobs][1].
    /// [1]: ./index.html#input-layers--blobs
    ///
    /// The input blobs are identified by name so they can be referenced as [bottom blobs][2]
    /// in a [LayerConfig][3].
    ///
    /// [2]: ../layer/index.html
    /// [3]: ../layer/struct.LayerConfig.html
    inputs: Vec<String>,

    /// Defines the [shape][1] of the [input blobs][2].
    /// [1]: ???
    /// [2]: ./index.html#input-layers--blobs
    ///
    /// The number of input_shapes supplied should match the number of inputs supplied.
    /// The shape of the input blobs has to be known so that the right connections to the
    /// upper layers can be set up.
    input_shapes: Vec<Vec<usize>>,

    /// Defines if the network will force every layer to do [backpropagation][1].
    /// [1]: https://en.wikipedia.org/wiki/Backpropagation
    ///
    /// If set to `false`, then the execution of backpropagation is determined automatically
    /// according to the net structure and learning rates.
    ///
    /// Default: `false`
    force_backward: bool,

    /// Defines the [state][1] of the network.
    /// [1]: ../struct.NetworkState.html
    ///
    /// Some layers may be included/excluded depending on this state and the states
    /// specified in the layers' include and exclude fields.
    pub state: NetworkState,

    /// Defines if the network will print debugging information about results
    ///
    /// Default: `false`
    debug_info: bool,

    /// Defines the layers of the network via [LayerConfig][1]s.
    /// [1]: ../layer/struct.LayerConfig.html
    pub layers: Vec<LayerConfig>,
}

impl Default for NetworkConfig {
    fn default() -> NetworkConfig {
        NetworkConfig {
            name: "".to_owned(),
            inputs: Vec::new(),
            input_shapes: Vec::new(),

            force_backward: false,
            debug_info: false,

            layers: Vec::new(),
            state: NetworkState::default(),
        }
    }
}

impl NetworkConfig {
    #[allow(missing_docs)]
    pub fn layer(&self, layer_id: usize) -> Option<&LayerConfig> {
        self.layers.get(layer_id)
    }

    #[allow(missing_docs)]
    pub fn input(&self, input_id: usize) -> Option<&String> {
        self.inputs.get(input_id)
    }

    #[allow(missing_docs)]
    pub fn input_shape(&self, input_id: usize) -> Option<&Vec<usize>> {
        self.input_shapes.get(input_id)
    }
}

#[derive(Debug)]
/// Defines the state of a network.
pub struct NetworkState {
    /// Defines the current mode of the network.
    ///
    /// Default: Test
    pub mode: NetworkMode,
    /// TODO: [DOC] what does this do?
    /// TODO: [DOC] could it be of type usize?
    ///
    /// Default: 0
    pub level: isize,
    /// TODO: [DOC] what does this do?
    ///
    /// Default: vec![]
    pub stage: Vec<String>,
}

impl Default for NetworkState {
    fn default() -> NetworkState {
        NetworkState {
            mode: NetworkMode::Test,
            level: 0,
            stage: vec![],
        }
    }
}

#[derive(Debug, Copy, Clone)]
/// Defines the possible modes that a network can be in.
pub enum NetworkMode {
    #[allow(missing_docs)]
    Train,
    #[allow(missing_docs)]
    Test,
}
