<!DOCTYPE HTML>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title></title>
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <base href="">

        <link rel="stylesheet" href="book.css">
        <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

        <link rel="shortcut icon" href="favicon.png">

        <!-- Font Awesome -->
        <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">

        <!-- MathJax -->
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Fetch JQuery from CDN but have a local fallback -->
        <script src="http://code.jquery.com/jquery-2.1.4.min.js"></script>
        <script>
            if (typeof jQuery == 'undefined') {
                document.write(unescape("%3Cscript src='jquery.js'%3E%3C/script%3E"));
            }
        </script>
    </head>
    <body>
        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme = localStorage.getItem('theme');
            if (theme == null) { theme = 'light'; }
            $('body').removeClass().addClass(theme);
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var sidebar = localStorage.getItem('sidebar');
            if (sidebar === "hidden") { $("html").addClass("sidebar-hidden") }
            else if (sidebar === "visible") { $("html").addClass("sidebar-visible") }
        </script>

        <div id="sidebar" class="sidebar">
            <ul class="chapter"><li><a href="./leaf.html"><strong>1.</strong> Leaf</a></li><li><a href="./layers.html"><strong>2.</strong> Layers</a></li><li><ul class="section"><li><a href="./layer-lifecycle.html"><strong>2.1.</strong> Layer Lifecycle</a></li><li><a href="./building-networks.html"><strong>2.2.</strong> Create a Network</a></li><li><a href="./create-new-layer.html"><strong>2.3.</strong> Create a new Layer</a></li></ul></li><li><a href="./solvers.html"><strong>3.</strong> Solvers</a></li><li><ul class="section"><li><a href="./model-training.html"><strong>3.1.</strong> Model Training</a></li><li><a href="./multi-device-model-training.html"><strong>3.2.</strong> Multi-Device Model Training</a></li><li><a href="./distributed-model-training.html"><strong>3.3.</strong> Distributed Model Training</a></li></ul></li><li><a href="./backend.html"><strong>4.</strong> Backend</a></li><li><a href="./deep-learning-glossary.html"><strong>5.</strong> Glossary</a></li><li class="spacer"></li><li><a href="http://autumnai.github.io/leaf/leaf/index.html"><strong>6.</strong> Rust API Documentation</a></li></ul>
        </div>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar" class="menu-bar">
                    <div class="left-buttons">
                        <i id="sidebar-toggle" class="fa fa-bars"></i>
                        <i id="theme-toggle" class="fa fa-paint-brush"></i>
                    </div>

                    <h1 class="menu-title"></h1>

                    <div class="right-buttons">
                        <i id="print-button" class="fa fa-print" title="Print this book"></i>
                    </div>
                </div>

                <div id="content" class="content">
                    <h1>Leaf</h1>
<p>This short book will teach you about <a href="https://github.com/autumnai/leaf">Leaf</a>, the Machine Intelligence Framework
engineered by software developers, not scientists. It was inspired by the
brilliant people behind TensorFlow, Torch,
Caffe, Rust and numerous research papers and brings modularity, performance and
portability to Deep Learning. Leaf has a very simple API, <a href="./layers.html">Layers</a> and
<a href="./solvers.html">Solvers</a>, and is one of the fastest Machine Intelligence Frameworks
available.</p>
<p><br/></p>
<div align="center">
  <iframe src="https://ghbtns.com/github-btn.html?user=autumnai&repo=leaf&type=star&count=true" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
  <a href="https://twitter.com/autumn_eng" class="twitter-follow-button" data-show-count="false">Follow @autumn_eng</a>
  <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
</div>
<p><br/></p>
<blockquote>
<p><strong>Assumption</strong><br />
The Leaf Book requires a basic understanding of the fundamental concepts
of Machine and Deep Learning. Recommended resources are</p>
<ul>
<li><a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning</a></li>
<li><a href="http://cs231n.github.io/">Stanford Course on (Convolutional) Neural Networks</a></li>
<li><a href="http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning/">A 'brief' history of Deep Learning</a></li>
<li><a href="./glossary.html">The Glossary</a></li>
</ul>
</blockquote>
<p>Deep Learning is really easy. Construct a network by chaining layers and then train the
network by feeding it examples. That is why Leaf's entire API
consists of only two concepts: <a href="./layers.html">Layers</a> and <a href="./solvers.html">Solvers</a>. Layers to
construct almost any kind of network. Deep Networks and even classical, stochastic based
algorithms/networks. And Solvers for training and execution of the network.
That is already the entire API for Machine Learning with Leaf.</p>
<h2>API Documentation</h2>
<p>Alongside this book you can also read the <a href="http://autumnai.github.io/leaf/">Rust API documentation</a> if
you would like to use Leaf as a crate or write a new library on top of it and
need a more low-level overview.</p>
<h2>License</h2>
<p>Leaf is licensed under either of</p>
<ul>
<li><a href="https://github.com/autumnai/leaf/blob/master/LICENSE-APACHE">Apache License v2.0</a> or,</li>
<li><a href="https://github.com/autumnai/leaf/blob/master/LICENSE-MIT">MIT license</a></li>
</ul>
<p>at your option.</p>
<h1>Layers</h1>
<h3>What are Layers?</h3>
<p>Layers represent functions. These functions can be mathematical expressions
like Sigmoid, ReLU, etc. or none mathematical instructions like querying data
from a database, logging data, etc. or anything in between. In Leaf, layers describe
not only the 'hidden layers', but also the input and output layer. In Leaf, as we
will see later on, everything is a layer, even the network itself. This makes the
API so clean and expressive.</p>
<p>Layers in Leaf are only slightly opinionated, in a sense, that they need to take
an input and produce an output. This is required in order to successfully stack
layers on top of each other and therefore create a network. Other than that, a
layer in Leaf can implement any behaviour.</p>
<p>In Leaf every layer can be constructed via the <a href="https://github.com/autumnai/leaf/blob/master/src/layer.rs"><code>LayerConfig</code>
(/src/layer.rs)</a>, which makes creating even complex networks easy
and manageable.</p>
<pre><code class="language-rust">// construct the config for a fully connected layer with 500 notes
let linear_1: LayerConfig = LayerConfig::new(&quot;linear1&quot;, LinearConfig { output_size: 500 })
</code></pre>
<p>A <code>LayerConfig</code> can be turned into an initialized, fully operable <a href="https://github.com/autumnai/leaf/blob/master/src/layer.rs"><code>Layer</code>
(/src/layer.rs)</a> with its <code>from_config</code> method.</p>
<pre><code class="language-rust">// construct the config for a fully connected layer with 500 notes
let linear_1: LayerConfig = LayerConfig::new(&quot;linear1&quot;, LinearConfig { output_size: 500 })
let linear_network_with_one_layer: Layer = Layer::from_config(backend, &amp;linear_1);
</code></pre>
<p>Hurray, we just have constructed our first network, as the network itself is
just a layer (a Leaf <code>Layer</code> struct).</p>
<p>The <code>from_config</code> method initializes a <code>Layer</code>, which has a worker field to which
it assigns the specific layer, (a struct that has <a href="https://github.com/autumnai/leaf/blob/master/src/layer.rs"><code>ILayer</code> (/src/layer.rs)</a> implemented).
In that tiny example above, the worker field of the <code>linear_network_with_one_layer</code>
is a <a href="https://github.com/autumnai/leaf/blob/master/src/layers/common/linear.rs"><code>Linear</code> (/src/layers/common/linear.rs)</a> as we constructed
the <code>linear_network_with_one_layer</code> from a <code>LinearConfig</code>. That worker field
introduces the specific behaviour of the layer.</p>
<p>In the next chapters of 2. Layers we explore more about how we can construct
real-world networks, the layer lifecycle and how we can add new layers to Leaf.</p>
<h3>What can Layers do?</h3>
<p>A layer can implement basically any behaviour, Deep Learning related like
convolutions or LSTM, classical Machine Learning related like nearest neighbors
or random forest or utility related like logging or normalization. To make the
behaviour of a layer more explicit Leaf groups layers into one of five
categories based on their (Machine Learning) functionality.</p>
<ol>
<li><a href="#Activation%20Layers">Activation</a></li>
<li><a href="#Common%20Layers">Common</a></li>
<li><a href="#Loss%20Layers">Loss</a></li>
<li><a href="#Utility%20Layers">Utility</a></li>
<li><a href="#Container%20Layers">Container</a></li>
</ol>
<p>In practice, the groups are of not much relevance. It helps making the file
structure cleaner, though. And it simplifies the explanation of what a layer is
doing.</p>
<h4>Activation Layers</h4>
<p>Activation layers provide element-wise operations and return an output of
the same size as the input. Activation layers can be seen as a synonym to
nonlinear <a href="https://en.wikipedia.org/wiki/Activation_function">Activation Functions</a>
and are a fundamental piece in Neural Networks.</p>
<p>Examples for Activation ;ayers are <code>Sigmoid</code>, <code>TanH</code> or <code>ReLU</code>. All available
activation layers can be found at
<a href="https://github.com/autumnai/leaf/tree/master/src/layers/activation">src/layers/activation</a>.</p>
<h4>Common Layers</h4>
<p>Common layers can differ in their connectivity and behavior and are typically
all network layer types which are not covered by activation or loss layers.</p>
<p>Examples for Common layers are <code>fully-connected</code>, <code>covolutional</code>, <code>pooling</code>, <code>LSTM</code>,
etc. All available common layers can be found at
<a href="https://github.com/autumnai/leaf/tree/master/src/layers/common">src/layers/common</a>.</p>
<h4>Loss Layers</h4>
<p>Loss layers compare an output to a target value and assign cost to minimize.
Loss layers are often the last layer in a model.</p>
<p>Examples for Loss layers are <code>Hinge Loss</code>, <code>Softmax Loss</code> or <code>Negative Log Likelihood</code>. All available loss layers can be found at
<a href="https://github.com/autumnai/leaf/tree/master/src/layers/loss">src/layers/loss</a>.</p>
<h4>Utility Layers</h4>
<p>Utility layers introduce all kind of helpful functionality, which might not be
directly related to machine learning and neural nets. This could be operations
for normalizing, restructuring or transforming information, log and debug
behavior or data access. Utility Layers follow the general behavior of a layer,
like the other types do.</p>
<p>Examples for Utility layers are <code>Reshape</code>, <code>Flatten</code> or <code>Normalization</code>. All
available utility layers can be found at
<a href="https://github.com/autumnai/leaf/tree/master/src/layers/utility">src/layers/utility</a>.</p>
<h4>Container Layers</h4>
<p>Container layers take <code>LayerConfig</code>s and connect them on initialization, which
creates a &quot;network&quot;. But as container layers are layers one can stack multiple
container layers on top of another and compose even bigger container layers.
Container layers differ in how they connect the layers that it receives.</p>
<p>Examples for Container layers are <code>Sequential</code>. All available container layers
can be found at
<a href="https://github.com/autumnai/leaf/tree/master/src/layers/container">src/layers/container</a>.</p>
<h3>Why Layers?</h3>
<p>The benefit of using a layer design approach is, that it allows for a very expressive
setup, which can represent even stochastic machine learning algorithms,
which makes Leaf usable in theory for almost any Machine Learning task not only
Deep Learning.</p>
<p>Other Machine Learning frameworks take a symbolic instead of a layered approach.
For Leaf we decided against it, as we found it easier for developers to handle
layers, than mathematical expressions. More complex algorithms e.g. LSTMs are
also harder to replicate in a symbolic framework than with layered ones. We
believe that Leafs layer approach strikes a great balance between,
expressiveness, usability and performance.</p>
<h1>Layer Lifecycle</h1>
<p>In <a href="./layers.html">2. Layers</a> we have already seen a little bit about how to
construct a <code>Layer</code> from a <code>LayerConfig</code>. In this chapter we take
a closer look on what happens inside Leaf when initializing a <code>Layer</code>, when
running the <code>forward</code> of a <code>Layer</code> and when running the <code>backward</code>. In the
next chapter <a href="./building-networks.html">2.2 Create a Network</a> we then
apply our knowledge to construct deep networks via the container layer.</p>
<p>Initialization (<code>from_config</code>), <code>forward</code> and <code>backward</code> are the three most
important methods of a <code>Layer</code> and describe basically the entire API. Now we
take a closer look on what happens inside Leaf, when these methods are called.</p>
<h3>Initialization</h3>
<p>A layer is constructed from a <code>LayerConfig</code> via the <code>Layer::from_config</code>
method, which returns a fully initialized <code>Layer</code>.</p>
<pre><code class="language-rust">let mut sigmoid: Layer = Layer::from_config(backend.clone(), &amp;LayerConfig::new(&quot;sigmoid&quot;, LayerType::Sigmoid))
let mut alexnet: Layer = Layer::from_config(backend.clone(), &amp;LayerConfig::new(&quot;alexnet&quot;, LayerType::Sequential(cfg)))
</code></pre>
<p>In the example above, the first layer has a Sigmoid worker
(<code>LayerType::Sigmoid</code>). The second layer has a Sequential worker.
Although both <code>Layer::from_config</code> methods, return a <code>Layer</code>, the behavior of
the <code>Layer</code> depends on the <code>LayerConfig</code> it was constructed with. The
<code>Layer::from_config</code> calls internally the <code>worker_from_config</code> method, which
constructs the specific worker defined by the <code>LayerConfig</code>.</p>
<pre><code class="language-rust">fn worker_from_config(backend: Rc&lt;B&gt;, config: &amp;LayerConfig) -&gt; Box&lt;ILayer&lt;B&gt;&gt; {
    match config.layer_type.clone() {
        // more matches
        LayerType::Pooling(layer_config) =&gt; Box::new(Pooling::from_config(&amp;layer_config)),
        LayerType::Sequential(layer_config) =&gt; Box::new(Sequential::from_config(backend, &amp;layer_config)),
        LayerType::Softmax =&gt; Box::new(Softmax::default()),
        // more matches
    }
}
</code></pre>
<p>The layer specific <code>::from_config</code> (if available or needed) then takes care of
initializing the worker struct, allocating memory for weights and so on.</p>
<p>In case the worker layer is a container layer, its <code>::from_config</code> takes
care of initializing all the <code>LayerConfig</code>s it contains (which were added via its
<code>.add_layer</code> method) and connecting them in
the order they were provided to the <code>LayerConfig</code> of the container.</p>
<p>Every <code>forward</code> or <code>backward</code> call that is now made to the returned <code>Layer</code> is send to
the worker.</p>
<h3>Forward</h3>
<p>The <code>forward</code> method of a <code>Layer</code> sends the input through the constructed
network and returns the output of the network's final layer.</p>
<p>The <code>forward</code> method does three things:</p>
<ol>
<li>Reshape the input data if necessary</li>
<li>Sync the input/weights to the device were the computation happens. This step
removes the worker layer from the obligation to care about memory synchronization.</li>
<li>Call the <code>forward</code> method of the worker layer.</li>
</ol>
<p>In case the worker layer is a container layer, the <code>forward</code> method of the
container layer takes care of calling the <code>forwad</code> methods of its managed
layers in the right order.</p>
<h3>Backward</h3>
<p>The <code>backward</code> of a <code>Layer</code> works quite similar to its <code>forward</code>. Although it
does not need to reshape the input. The <code>backward</code> computes
the gradient with respect to the input and the gradient w.r.t. the parameters but
only returns the gradient w.r.t the input as only that is needed to compute the
gradient of the entire network via the chain rule.</p>
<p>In case the worker layer is a container layer, the <code>backward</code> method of the
container layer takes care of calling the <code>backward_input</code> and <code>backward_parameter</code>
methods of its managed layers in the right order.</p>
<h1>Create a Network</h1>
<p>In the previous chapters, we learned that everything is a layer. Even the network
itself is a layer and therefore behaves like any other layer which means,
that it could be used to create even bigger networks. This is possible, because
a <code>Layer</code> can implement any behavior as long as it takes an input and produces
an output. In <a href="./layer-lifecycle.html">2.1 Layer Lifecycle</a>
we have seen, that only one <code>LayerConfig</code> can be used to turn it via
<code>Layer::from_config</code> into an actual <code>Layer</code>. But as Deep Learning relies on
chaining multiple layers together, we need a <code>Layer</code>, who implements this
behavior for us. Enter the container layers.</p>
<h3>Networks via the <code>Sequential</code> layer</h3>
<p>A <code>Sequential</code> is a layer of the container layer category. The config of a
container layer, e.g. <code>SequentialConfig</code> has a special method called,
<code>.add_layer</code> which takes one <code>LayerConfig</code> and adds it to an ordered list in the
<code>SequentialConfig</code>.</p>
<p>When turning a <code>SequentialConfig</code> into a <code>Layer</code> by passing the config to
<code>Layer::from_config</code>, the behavior of the Sequential is to initialize all the
layers which were added via <code>.add_layer</code> and connect the layers with each other.
This means, the output of one layer becomes the input of the next layer in the
list. The input of a <code>Layer</code> with a sequential worker, becomes the input of the
first layer in the sequential worker, the sequential worker then takes care
of passing the input through all the layers and the output of the last layer
then becomes the output of the <code>Layer</code> with the sequential worker. Therefore
a sequential <code>Layer</code> fulfills the requirements of a <code>Layer</code>.</p>
<pre><code class="language-rust">// short form for: &amp;LayerConfig::new(&quot;net&quot;, LayerType::Sequential(cfg))
let mut net_cfg = SequentialConfig::default();

net_cfg.add_input(&quot;data&quot;, &amp;vec![batch_size, 28, 28]);
net_cfg.add_layer(LayerConfig::new(&quot;reshape&quot;, ReshapeConfig::of_shape(&amp;vec![batch_size, 1, 28, 28])));
net_cfg.add_layer(LayerConfig::new(&quot;conv&quot;, ConvolutionConfig { num_output: 20, filter_shape: vec![5], stride: vec![1], padding: vec![0] }));
net_cfg.add_layer(LayerConfig::new(&quot;pooling&quot;, PoolingConfig { mode: PoolingMode::Max, filter_shape: vec![2], stride: vec![2], padding: vec![0] }));
net_cfg.add_layer(LayerConfig::new(&quot;linear1&quot;, LinearConfig { output_size: 500 }));
net_cfg.add_layer(LayerConfig::new(&quot;sigmoid&quot;, LayerType::Sigmoid));
net_cfg.add_layer(LayerConfig::new(&quot;linear2&quot;, LinearConfig { output_size: 10 }));
net_cfg.add_layer(LayerConfig::new(&quot;log_softmax&quot;, LayerType::LogSoftmax));

// set up the sequential layer aka. a deep, convolutional network
let mut net = Layer::from_config(backend.clone(), &amp;net_cfg);
</code></pre>
<p>As a sequential layer is like any other layer, we can use sequential layers as
building blocks for larger networks. Important building blocks of a network can
be grouped into a sequential layer and published as a crate for others to use.</p>
<pre><code class="language-rust">// short form for: &amp;LayerConfig::new(&quot;net&quot;, LayerType::Sequential(cfg))
let mut conv_net = SequentialConfig::default();

conv_net.add_input(&quot;data&quot;, &amp;vec![batch_size, 28, 28]);
conv_net.add_layer(LayerConfig::new(&quot;reshape&quot;, ReshapeConfig::of_shape(&amp;vec![batch_size, 1, 28, 28])));
conv_net.add_layer(LayerConfig::new(&quot;conv&quot;, ConvolutionConfig { num_output: 20, filter_shape: vec![5], stride: vec![1], padding: vec![0] }));
conv_net.add_layer(LayerConfig::new(&quot;pooling&quot;, PoolingConfig { mode: PoolingMode::Max, filter_shape: vec![2], stride: vec![2], padding: vec![0] }));
conv_net.add_layer(LayerConfig::new(&quot;linear1&quot;, LinearConfig { output_size: 500 }));
conv_net.add_layer(LayerConfig::new(&quot;sigmoid&quot;, LayerType::Sigmoid));
conv_net.add_layer(LayerConfig::new(&quot;linear2&quot;, LinearConfig { output_size: 10 }));

let mut net_cfg = SequentialConfig::default();

net_cfg.add_layer(conv_net);
net_cfg.add_layer(LayerConfig::new(&quot;linear&quot;, LinearConfig { output_size: 500 }));
net_cfg.add_layer(LayerConfig::new(&quot;log_softmax&quot;, LayerType::LogSoftmax));

// set up the 'big' network
let mut net = Layer::from_config(backend.clone(), &amp;net_cfg);
</code></pre>
<h3>Networks via other container layers</h3>
<p>So far, there is only the sequential layer, but other container layers, with
slightly different behaviors are conceivable. For example a parallel or
concat layer in addition to the sequential layer.</p>
<h1>Create a new Layer</h1>
<h1>Solvers</h1>
<h1>Model Training</h1>
<h1>Multi-Device Model Training</h1>
<h1>Distributed Model Training</h1>
<h1>Backend</h1>
<h1>Glossary</h1>
<h3>Network</h3>
<p>A network is a graph of one or many layers. An initialized network is a network,
which is ready to be executed, meaning it is fully constructed and all necessary
memory is allocated on the host or device.</p>
<h1>Rust API Documentation</h1>

                </div>

                <!-- Mobile navigation buttons -->
                

                

            </div>

            

            

        </div>


        <!-- Local fallback for Font Awesome -->
        <script>
            if ($(".fa").css("font-family") !== "FontAwesome") {
                $('<link rel="stylesheet" type="text/css" href="_FontAwesome/css/font-awesome.css">').prependTo('head');
            }
        </script>

        <script src="highlight.js"></script>
        <script src="book.js"></script>
    </body>
</html>
